Giacomo Di Liberto, Russell Silva, Kristina Nemeth, Michelle Feng, Joan Song
ECE 3400 Ethics Assignment
11/20/2017
Ethics: AI Drones and Warfare

### Introduction
Artificial intelligence is defined as the ability for a machine “to perform tasks commonly associated with intelligent beings.”1 Examples of this technology include SIRI, facial recognition and self-driving cars. The recent rise in the use of unmanned and autonomous systems, such as drones and vehicles, has shed light on another potential use for AI: warfare. The development of what some call “killer” robots has led to questions of whether or not intelligent weapons should be designed at all, and, if they were, what their potential impact would be.2 A recent open letter titled “Research Priorities for Robust and Beneficial Artificial Intelligence”  endorsed by Elon Musk, Stephen Hawking, and leading AI researchers posit the belief that continued development of AI weaponry would lead to a global AI arms race, if any major military force adopted it in a meaningful capacity.3 Based on the tone and content of the letter, it seems that Musk et al. believe that technological development in AI is inevitable; however, this growth will quickly facilitate the development of autonomous weapons that do not possess the same restrictions as “meaningful human control”. 3Once this occurs, an artificial intelligence arms race similar to that of the nuclear arms race will also proceed.

### Stakeholders
Although artificial intelligence weapons will be designed to destruct, there are stakeholders who would benefit from such an arms race. Governments fighting active wars may want to obtain or develop AI weapons to gain an advantage over their opponents; they may also want to utilize the weapons to deter against foreign invasions and coups. These weapons would presumably be more efficient than current weapons of destruction.  Additionally, with the advent of AI weaponry, fewer active military personnel will be required to participate in combat, therefore fewer potential casualties.

Companies that develop AI technology also stand to make a lot of money from AI weapons.  In 2015, the United States government paid the top government contracting companies 175.1 billion dollars. 4 An AI arms race would potentially only increase the amount of money paid to these companies as the cost of artificial intelligence weapons is presumably higher. Despite the benefits to certain stakeholders, it is important to note that everyone is affected when discussing the topic of AI weapons. This means that anyone not in the government, military, or a government-contracting company, is also a potential stakeholder, because the effects of AI warfare and weaponry would be widespread. Additionally, the stakeholders who may benefit from the development of AI weapons may also be negatively impacted by the use of such weapons. Overall, the use of AI warfare has the potential to harm and kill many people. 
To summarize, we have four main stakeholders: governments (those with significant military and political power), military personnel (those who would go to war), civilians (those who would be harmed) and government contracting companies (those that stand to make a lot of money from AI weapons).

### Ethics Tests
We believe the utilitarian test would apply best in this situation, as the utilitarian test is based off of the idea that decisions can be made based off of the best possible outcome and that all stakeholders, both current and future, should be taken into account. This is especially applicable in this scenario because the development of AI weapons will not only impact everyone in the present but also future generations. We see that the number of civilians that could be put at harm by these AI weaponry outweigh those who gain material wealth from the development of this research.  The justice test may also be applicable in the discussion because it takes into account whether or not the distribution of burdens and gains from an action is fair or not. However, we felt like it would be difficult to determine a “fair” outcome, especially when talking about war.  Finally, we feel that the character/virtue test would be difficult to apply  in this situation because it may not be valid for us to establish a moral vision for everyone in the world.  Additionally, the morality of war itself is already questionable, let alone the development of AI weapons. 

### Application and Solution
When applying the utilitarian test, it’s necessary to ask: will AI weaponry and warfare benefit current and future stakeholders? There is not necessarily a clear answer to that. Governments utilizing sophisticated AI weapons can secure themselves domestic and international security, as well as international prominence --  such influence could potentially lend itself well not only politically but also economically, granted that these weapons are used in positive ways such as peacekeeping methods and ways to fight terrorism. In this case, the government would not only benefit the government but also civilians and the general public. Those serving in the military --especially frontliners and foot soldiers- would be benefitted, as combat time and the number of servicemen killed in combat would be reduced. The use of these weapons would clearly also be beneficial to government contracting companies as they would have larger earnings.
Unfortunately, just as a government with AI weaponry may have the power to maintain peace internationally, it will also have the power to cause large-scale destruction. From the history human politics and war, we predict that peace may be maintained in the short-term by a government with AI weaponry, however, warfare using AI technology may likely occur in the long term.5AI weaponry and warfare has the potential to harm all current and future stakeholders. The use of this technology will have the capacity to kill any person who it is used against. Although current warfare already is responsible for many deaths worldwide, the use of AI weapons can only increase the death toll. Overall, we believe the unregulated development of AI weaponry will not benefit very few, if any, stakeholders in the long term, and any small benefits will be deeply undermined by the large potential scale of destruction.
We believe an ideal outcome would be a complete stop in the development of AI weaponry. Peace would benefit all current and future stakeholders and the development of more weapons is in the end counterproductive. However, we also believe this outcome is very unlikely, especially in the long-term. There are many stakeholders, governments and government contracting companies, who stand to gain a lot of money and political influence from the creation of AI weapons. Additionally, people may feel a need to develop AI weapons as a means of self-preservation. People may think the existence of such weapons is inevitable because others are working to develop it; therefore, why not create AI weapons first to gain the upper ground? This mentality is likely to exist already among various governments because it was this thought process which fueled the race to develop nuclear weapons during WWII.6
We believe that an alternative action would be the creation of an international committee to regulate the development and sale of AI weapons. This way, it may be possible to use economic sanctions to restrain rogue nations that intend to cause mass destruction with AI weapons.  The UN already plays a role in attempting to regulate and eliminate nuclear
weapons.7Therefore, we believe such a regulatory body is possible.  It should be noted that the UN was established after WWII and the use of nuclear weapons by the United States in Japan. Our hope is that a regulatory body would be created now, before further development and use of AI weapons. There are, of course, problems with this alternative action-- governments may be unwilling to participate. However, we believe this outcome would benefit many current and future stakeholders (ie. everyone) by attempting to control the destructive capacity of AI weapons. Additionally, a well organized regulatory body may be able to insure both the short-term and long-term benefits to stakeholders. 
