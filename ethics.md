Giacomo Di Liberto, Russell Silva, Kristina Nemeth, Michelle Feng, Joan Song
ECE 3400 Ethics Assignment
11/20/2017

## Ethics: AI Drones and Warfare

### Introduction
Artificial intelligence is defined as the ability for a machine “to perform tasks commonly associated with intelligent beings.”<sup>1</sup> Examples of this technology include SIRI, facial recognition and self-driving cars. The recent rise in the use of unmanned and autonomous systems, such as drones and vehicles, has shed light on another potential use for AI: warfare. The development of what some call “killer” robots has led to questions of whether or not intelligent weapons should be designed at all, and, if they were, what their potential impact would be.<sup>2</sup> A recent open letter titled “Research Priorities for Robust and Beneficial Artificial Intelligence”  endorsed by Elon Musk, Stephen Hawking, and leading AI researchers posits the belief that continued development of AI weaponry would lead to a global AI arms race, if any major military force adopted it in a meaningful capacity.<sup>3</sup> Based on the tone and content of the letter, it seems that Musk et al. believe that technological development in AI is inevitable; however, this growth will quickly facilitate the development of autonomous weapons that do not possess the same restrictions as “meaningful human control”.<sup>3</sup> Once this occurs, an artificial intelligence arms race similar to that of the nuclear arms race will also proceed.

### Stakeholders
Although artificial intelligence weapons will be designed to destruct, there are stakeholders who would benefit from such an arms race. Governments fighting active wars may want to obtain or develop AI weapons to gain an advantage over their opponents; they may also want to utilize the weapons to deter against foreign invasions and coups. These weapons would presumably be more efficient than current weapons of destruction.  Additionally, with the advent of AI weaponry, fewer active military personnel will be required to participate in combat, therefore fewer potential casualties.

Companies that develop AI technology also stand to make a lot of money from AI weapons.  In 2015, the United States government paid the top government contracting companies 175.1 billion dollars.<sup>4</sup> An AI arms race would potentially only increase the amount of money paid to these companies as the cost of artificial intelligence weapons is presumably higher. Despite the benefits to certain stakeholders, it is important to note that everyone is affected when discussing the topic of AI weapons. This means that anyone not in the government, military, or a government-contracting company, is also a potential stakeholder, because the effects of AI warfare and weaponry would be widespread. Additionally, the stakeholders who may benefit from the development of AI weapons may also be negatively impacted by the use of such weapons. Overall, the use of AI warfare has the potential to harm and kill many people. 

To summarize, we have four main stakeholders: governments (those with significant military and political power), military personnel (those who would go to war), civilians (those who would be harmed) and government contracting companies specializing into AI (those that stand to make a lot of money from AI weapons).

### Ethics Tests
When discussing AI weaponry, it’s important to consider the three ethics tests: the justice, utilitarian, and virtue tests. The justice test asks if there is a fair distribution of benefits and burdens when it comes to all stakeholders. The utilitarian asks whether or not everyone will benefit from the development of AI weapons, and the virtue test asks if such weapons would represent the values of the organizations that develop and/or utilize them.

The justice test is applicable in the discussion because it takes into account whether or not the distribution of burdens and gains from an action is fair or not. For example, the justice test could come into play when looking at the possession of AI weapons by one country. That country and it’s civilians may then possess all the “gains” from the development of AI weaponry while all others would be left to carry the “burdens.”  Such an outcome could question a fair distribution of benefits and burdens. It might not be considered fair for only a fraction of the world population to gain from the development of AI weapons while the rest of the world suffers for the use of AI warfare. However, one can argue that is fair that the inventors of AI weapons reap the benefits.  Overall, it would be difficult to determine a “fair” outcome in this scenario, because a war scenario commonly involves an unfair balance of resources among aggressors. Therefore, it would be difficult to justify an equal distribution in a scenario with a myriad of benefits/drawbacks for each country for each action. 

A character/virtue test would also be difficult to apply in this situation because establishing a moral vision for everyone in the world would be unfeasible when taking into account every known culture and belief that exists today.  The vast difference of opinion that would arise from discerning “humanity’s reputation or vision” would make a character/virtue test fail or contradict itself in every possible outcome. Additionally, the morality of war itself is already questionable, let alone the development of AI weapons.  If we were to try to apply the virtue test to this topic, we may want to start with the assumption that people are inherently good. If we were to continue with this assumption, then the creation of AI weapons would not fit into the moral standard of humanity because AI weapons are designed to harm. 

We believe the utilitarian test would apply best in this situation, as the utilitarian test is based off the idea that decisions can be made based off of the best possible outcome and that all stakeholders, both current and future, should be taken into account. This is especially applicable in this scenario because the development of AI weapons will not only impact everyone in the present but also future generations. We see that the number of civilians that could be put at harm by these AI weaponry outweigh those who gain material wealth from the development of this research. More on this is discussed in the following section.


### Solutions & Economic, Social, and Political Constraints
Do the economic, social, and political benefits of AI weaponry outweigh the cons of such technology? There is not necessarily a clear answer to that. Governments utilizing sophisticated AI weapons can secure themselves in domestic and international security, as well as international prominence. Such influence could potentially lend itself well not only politically but also economically, granted that these weapons are used in positive ways such as maintaining peace. In this case, AI technology would not only benefit those in the government but also civilians and the general public. Those serving in the military --especially frontliners and foot soldiers-- would also benefitted, as combat time and the number of servicemen killed in combat would be reduced. The development of these weapons would also be beneficial to research and engineering job growth within government contracting companies.

Unfortunately, just as a government with AI weaponry may have the power to maintain peace internationally, it will also have the power to cause large-scale destruction. From the history of human politics and war,  we can see that times of peace have always been interrupted with war. Therefore, we predict that peace may be maintained in the short-term by a government with AI weaponry; however, warfare using AI technology may likely occur in the long term.<sup>5</sup>AI weaponry and warfare has the potential to harm all current and future stakeholders. The use of this technology will have the capacity to kill any person within its proximity. Although current warfare is already responsible for many deaths worldwide, the use of AI weapons can only increase the death toll. Overall, we believe the unregulated development of AI weaponry will not benefit many, if any, stakeholders in the long term, and any small benefits will be deeply undermined by the large potential scale of destruction.

We believe an ideal outcome would be a complete stop in the development of AI weaponry. Peace would benefit all current and future stakeholders and the development of more weapons is in the end counterproductive. However, we also believe this outcome is very unlikely, especially in the long-term due to social, political and economic constraints. There are many stakeholders, governments and government contracting companies, who stand to gain a lot of money and political influence from the creation of AI weapons. Additionally, people may feel a need to develop AI weapons as a means of self-preservation. People may think the existence of such weapons is inevitable because others are working to develop it; therefore, why not create AI weapons first to gain the upper ground? This mentality likely already exists among various governments because it was this thought process which fueled the race to develop nuclear weapons during and after WWII.<sup>6</sup>

An alternative action would be the creation of an international committee to regulate the development and sale of AI weapons. This way, it may be possible to use economic sanctions to restrain rogue nations that intend to cause mass destruction with AI weapons.  The UN already plays a role in attempting to regulate and eliminate nuclear weapons<sup>7</sup>-therefore, creating an AI-based regulation committee should be possible as well. It should be noted that the UN was established after WWII and the use of nuclear weapons by the United States in Japan. Our hope is that a regulatory body would be created now, before further development and use of AI weapons. There are, of course, constraints to this alternative action-- governments may be unwilling to participate and investing countries may be unwilling to forfeit a potential economic opportunity. However, we believe this outcome would benefit many current and future stakeholders (ie. everyone) by attempting to control the destructive capacity of AI weapons. Additionally, a well organized regulatory body may be able to ensure both the short-term and long-term benefits to stakeholders. 

### Works Cited
1. Copeland, B.J. “Artificial Intelligence (AI).” Encyclopædia Britannica, Encyclopædia Britannica, Inc., 12 Jan. 2017, www.britannica.com/technology/artificial-intelligence
2. Lien, Tracey. “Elon Musk and AI Experts Urge U.N. to Ban Artificial Intelligence in Weapons.” Los Angeles Times, 21 Aug. 2017, www.latimes.com/business/technology/la-fi-tn-musk-killer-robots-20170821-story.html.
3. “Open Letter on Autonomous Weapons.” Future of Life Institute, 28 July 2015, futureoflife.org/open-letter-autonomous-weapons/.
4. Mehta, Aaron. “Lockheed Martin Biggest US Government Contractor in 2015.” Defense News, Defense News, 8 Aug. 2017, www.defensenews.com/industry/2016/05/09/lockheed-martin-biggest-us-government-contractor-in-2015/.
5. “Timeline of Wars.” Timeline of War, www.datesandevents.org/events-timelines/24-timeline-of-war.htm.
6. History.com Staff. “Arms Race.” History.com, A&E Television Networks, 2009, www.history.com/topics/cold-war/arms-race.
7. “United Nations Conference to Negotiate and Legally Binding Instrument to Prohibit Nuclear Weapons, Leading Towards Their Total Elimination, 27 April to 22 May 2015.” United Nations, United Nations, www.un.org/disarmament/ptnw/.

